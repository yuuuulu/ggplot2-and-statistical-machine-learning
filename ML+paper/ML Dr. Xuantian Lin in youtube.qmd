---
title: "ML"
---

# Thanks to Dr. Mu He for recommending @ Machine Learning Foundations (機器學習基石) to me.

## PLA
simple, fast, and used to each demension

### Introduction

The Perceptron Learning Algorithm (PLA) is a fundamental algorithm in machine learning for binary classification. This document provides a detailed proof of the convergence of the PLA, demonstrating that the algorithm will halt after a finite number of updates if the data is linearly separable.

### Key Definitions and Assumptions

We start with the following assumptions and definitions:

-   **Linearly Separable Data**: There exists a weight vector ( w\^\* ) and a bias ( b\^\* ) such that for all training examples ((x_i, y_i)):

    $$
    y_i (w^* \cdot x_i + b^*) > 0
    $$

-   **Normalized Weight Vector**: Normalize ( w\^\* ) such that ( \|\|w\^\*\|\| = 1 ).

### Perceptron Algorithm Updates

The Perceptron Learning Algorithm updates the weight vector ( w ) and bias ( b ) according to the following rule:

$$
w_{t+1} = w_t + y_i x_i
$$ $$
b_{t+1} = b_t + y_i
$$

Margin Definition

Define the margin (\gamma) as:

$$
\gamma = \min_{i} \frac{y_i (w^* \cdot x_i)}{||x_i||}
$$

This is the minimum distance from the data points to the decision boundary defined by ( w\^\* ). Mistake-Driven Updates

The weight vector ( w_t ) is changed only when a mistake is made, i.e., when:

$$
\text{sign}(w_t \cdot x_{n(t)}) \neq y_{n(t)} \implies y_{n(t)} w_t^T x_{n(t)} \leq 0
$$

Bounding the Norm of ( w_t )

A mistake "limits" the growth of ( \|\|w_t\|\|\^2 ), even when updating with the "longest" ( x_n ). Consider the update rule:

$$
||w_{t+1}||^2 = ||w_t + y_{n(t)} x_{n(t)}||^2
$$ $$
= ||w_t||^2 + 2 y_{n(t)} w_t^T x_{n(t)} + ||y_{n(t)} x_{n(t)}||^2
$$ $$
\leq ||w_t||^2 + 0 + ||y_{n(t)} x_{n(t)}||^2
$$ $$
\leq ||w_t||^2 + \max_n ||y_n x_n||^2
$$

Starting from ( w_0 = 0 )

Starting from ( w_0 = 0 ), after ( T ) mistake corrections, we have:

$$
\frac{w_T \cdot w^*}{||w_T||} \geq \sqrt{T} \cdot \text{constant}
$$

Dot Product Analysis

Analyze the dot product of ( w_t ) with ( w\^\* ):

$$
w_{t+1} \cdot w^* = (w_t + y_i x_i) \cdot w^*
$$ $$
= w_t \cdot w^* + y_i (w^* \cdot x_i)
$$ $$
\geq w_t \cdot w^* + \gamma ||x_i||
$$

Bounding the Number of Mistakes

Using the above inequalities, we can bound the number of updates ( t ) needed to achieve convergence. From the dot product analysis:

$$
w_t \cdot w^* \geq t \gamma ||x_i||
$$

From the norm analysis:

$$
||w_t||^2 \leq t ||x_i||^2
$$

Combining these, we get:

$$
\frac{w_t \cdot w^*}{||w_t||} \geq \frac{t \gamma ||x_i||}{\sqrt{t} ||x_i||}
$$ $$
\Rightarrow \sqrt{t} \leq \frac{||w^*||}{\gamma}
$$ $$
\Rightarrow t \leq \left( \frac{||w^*||}{\gamma} \right)^2
$$

Since ( \|\|w\^\*\|\| = 1 ), we get:

$$
t \leq \frac{1}{\gamma^2}
$$

### Conclusion

The proof shows that the Perceptron Learning Algorithm will make a finite number of mistakes (updates) before converging to a solution that correctly classifies all the training data, provided the data is linearly separable. The number of updates is bounded by ( \frac{1}{\gamma^2} ), where ( \gamma ) is the margin of separation between the classes. This formal analysis demonstrates the convergence and correctness of the iterative process used by the PLA.

#### Explanation of Constant in the video

The term `constant` in the equation

$$
\frac{w_T \cdot w^*}{||w_T||} \geq \sqrt{T} \cdot \text{constant}
$$

refers to a value that depends on the characteristics of the data, particularly the margin (\gamma). The margin (\gamma) is the minimum distance from any data point to the decision boundary. This constant reflects how well-separated the classes are in the feature space. In practical terms, a larger margin (or greater separation) leads to a smaller constant, implying that the algorithm will converge more quickly.



# Why could machine learn? 
The Theorem of Generalization ensures it by guaranteeing $E_in$ approximately equal to $E_out$, which could be small. We could explain it from the very beginning which is an interesting and logical process.



## simple intro first


Unknown target function:

$$
f : \mathcal{X} \rightarrow \mathcal{Y}
$$

Training examples:

$$
D : (x_1, y_1), \cdots, (x_N, y_N)
$$

Hypothesis set:

$$
\mathcal{H}
$$

Learning algorithm:

$$
\mathcal{A}
$$

Unknown distribution on \( \mathcal{X} \):

$$
P
$$

Final hypothesis approximating \( f \):

$$
g \approx f
$$

'Worst case' guarantee on generalization


## target f and g
## eg of 2D pr
## PAL based on concentration ..
 

$$
\forall g = \mathcal{A}(\mathcal{D}) \in \mathcal{H} \text{ and 'statistical' large } \mathcal{D}, \text{ for } N \ge 2, k \ge 3
$$

$$
\mathbb{P}_{\mathcal{D}} \left[ | E_{\text{in}}(g) - E_{\text{out}}(g) | > \epsilon \right]
\le
\mathbb{P}_{\mathcal{D}} \left[ \exists h \in \mathcal{H} \text{ s.t. } | E_{\text{in}}(h) - E_{\text{out}}(h) | > \epsilon \right]
\le
4 m_{\mathcal{H}}(2N) \exp \left( -\frac{1}{8} \epsilon^2 N \right)
$$

$$
\text{if } k \text{ exists}
\le
4 (2N)^{k-1} \exp \left( -\frac{1}{8} \epsilon^2 N \right)
$$


This leads us to VC dimension(and above is acctually a VC Bound if we replace k-1 with $d_{vc}$:


# How does machine learn?----- VC dimension
( $d_{vc}$= $min_{k-1}$)
## connection with the last part
# Generalization with Finite $d_{vc}$ 

Finite $d_{vc}$ implies that \( g \) 'will' generalize:

$$
E_{\text{out}}(g) \approx E_{\text{in}}(g)
$$

This holds:
- Regardless of learning algorithm \( \mathcal{A} \)
- Regardless of input distribution \( P \)
- Regardless of target function \( f \)






## VC demension of perceptrons

### proof of $d_{vc}=d+1$
(d is the number of the demension of the perceptrons):
method:  $d_{vc}\leq d+1$ and $d_{vc}\geq d+1$

#### firstly, $d_{vc}\geq d+1$
i.e. there exists some d + 1 inputs we can shatter ( reason: $d_{vc}$= $k_{min}-1$ so k $\geq  $ d+2 )

We use this special data to show the existance(d demensional space sattered by d+1 points(vectors)):
 $$
X = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
1 & 1 & 0 & \cdots & 0 \\
1 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & 0 & 0 & \cdots & 1
\end{bmatrix}
$$
(*the demension of the perceptron is not the same as the dimensions of a matrix!---so do not confused about sth wasting a lot of time----focus more on the define then maybe we will be more effective*)


col-1: origin
col-2: 1-d 1 weight
col-3:2-d 1 weight
...
     ----1 data of origin, d datas ---d+1 data 


### ps: I just did not understand shatter very well until now: <2^n means the ability of a hypothesis class to perfectly classify every possible combination of labels for a set of points. 

So we come back to the proof: d points shatter d+1 dimensions means for any y in d+1 dimensions, there exists a vector w such that sign(Xw)=y. So if we could prove Xw=y, we could succeed. Since X is invertible, w=$X^{-1}y$



#### secondly,  $d_{vc}\leq d+1$ 

i.e. We cannot shatter any set of d+2 inputs.

Since we just used d+1 points in the last proof, we only need to add one point now. If we add a vector that is linear dependent with the d+1 points(here we use linear dependence to restrict dichotomy). This offers us some instructions.



proof: d+2 rows > d+1 columns---linear dependent----x_d+2 could be the linear combination of those d+1 points

 So we will prove :if shatters, For any d+2 points, among the 2^{d+2} situations you want to test, there will be a situation where a_i and w^T w_i have the same sign but  the d+2th  situation  is absolutely impossible happends  at this point.
  
  Suppose it could be shatter(proof of contradiction)：
  
  
  (I did not understand it so i will not explain until one day i understand)
  
  
  
## M and d_vc---degree of freedom

$d_{vc}=d+1:$effective 'binary' degrees of freedom, powerfulness of H.


Since $M=2(2N)^{d_{vc}}$ 
Small M has too few choices so that A may not choose small enough E_in(g); Similarly, small $d_{vc}$ has too limited power.
Large M (d_vc) may not make sure E_out(g) and E_in(g) close enough;Similarly, large $d_{vc}$, either.
  
## VC Bound Rephrase
a rephrase from  PAC (Probably Approximately Correct) :
 $$ E_{in}(g) - \sqrt{\frac{8}{N} \ln\left(\frac{4(2N)^{d_{VC}}}{\delta}\right)}\leq E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{8}{N} \ln\left(\frac{4(2N)^{d_{VC}}}{\delta}\right)}$$ where $\delta=4 (2N)^{k-1} \exp \left( -\frac{1}{8} \epsilon^2 N \right)$ and g=A(D)in H and statistical large D with $d{vc}\geq 2$ 
the square-root is the penalty for model complexity(relevant to:N(how many points) H(what is the VC dimension d_vc) $\delta$(how lucky you think you are?????))

  
  
  
  

